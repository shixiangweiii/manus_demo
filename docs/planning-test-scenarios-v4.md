## Manus Demo v4 自主规划测试用例集（按难度分级）

> 本文档提供一组 **简单 / 中等 / 困难** 难度的任务场景，用于手工验证 Manus Demo v4（混合规划路由）的：
>
> - **规划正确性**：分类是否合理、计划结构是否匹配任务复杂度
> - **执行速度**：简单任务是否显著更快，复杂任务是否能发挥 DAG 并行优势
> - **最终结果**：输出内容是否正确、有用且解释清晰

---

## 一、如何运行这些用例

- **运行方式**：
  - 交互模式：在仓库根目录执行：
    - `PLAN_MODE=auto python main.py`
    - 按用例中的「任务描述」逐条输入。
  - 单次任务模式：直接在命令行传入任务：
    - `PLAN_MODE=auto python main.py "在这里粘贴任务描述"`
- **PLAN_MODE 说明**（详见 `config.py` 与 `docs/hybrid-plan-routing-v4.md`）：
  - `auto`（默认）：启用 v4 两阶段混合分类器，自动路由到 simple / complex 路径。
  - `simple`：强制走 v1 扁平计划路径（用于调试简单路径）。
  - `complex`：强制走 v2 DAG 路径（用于调试复杂路径）。
- **终端可观测信号**（来自 `main.py` 的 Rich UI 和 orchestrator 事件）：
  - `Task complexity: Simple (v1 flat plan)` 或 `Task complexity: Complex (v2 DAG)`。
  - Simple 路径下的 `Simple Plan (v1)` 表格（步数、依赖）。
  - Complex 路径下的 DAG 树形结构及节点状态（PENDING/READY/RUNNING/COMPLETED/FAILED/SKIPPED）。
  - 每个节点执行时的工具调用日志（`web_search` / `execute_python` / `file_ops`）。
  - Reflection 面板中的评分与反馈。

执行这些用例时，建议先整体感受：**简单 → 中等 → 困难** 难度递增时，规划结构、执行轮次（Super-step 数量）和延迟的变化趋势。

---

## 二、测试维度与观测点

结合 `docs/hybrid-plan-routing-v4.md` 与 `docs/data-structures-and-algorithms.md`，本套用例重点覆盖：

- **路由与分类**：
  - `_rule_classify()` 的规则是否能正确区分明显简单 / 明显复杂任务。
  - 对于模棱两可任务，是否只在必要时触发 `_llm_classify()`，并给出合理判定。
- **规划结构**：
  - Simple 路径：2–6 步的扁平计划，线性依赖是否清晰合理。
  - Complex 路径：是否生成包含 Goal → SubGoals → Actions 的三层 DAG，且依赖合理。
- **执行行为**：
  - Simple 路径：顺序执行，反思阶段是否能在一次或少量重规划后通过。
  - Complex 路径：Super-step 并行度是否合理；条件边、回滚边是否被正确触发。
- **错误恢复与结果质量**：
  - 故意注入失败点时，失败节点及其下游是否被 SKIPPED 或回滚。
  - 在存在局部失败的情况下，最终回答是否仍然有价值并解释失败原因。
- **执行速度（定性）**：
  - 同等硬件和模型配置下，简单用例的整体延迟应明显低于困难用例。
  - 中等用例中，Simple 路径通常更快；Complex 路径会换取更清晰的分解和更强的容错。

---

## 三、用例总览

| 用例 ID | 难度 | 任务一句话描述 | 推荐 PLAN_MODE | 预期路径 | 特性备注 |
|--------|------|----------------|----------------|----------|----------|
| S1 | 简单 | 查询上海今天天气 | `auto` | Simple v1 | 单步查询 |
| S2 | 简单 | 计算 12345+67890 的结果 | `auto` | Simple v1 | 纯计算 |
| S3 | 简单 | 用一句话总结 Python 列表推导式的作用 | `auto` | Simple v1 | 简短概念总结 |
| S4 | 简单 | 将一句中文翻译为英文 | `auto` | Simple v1 | 翻译任务 |
| S5 | 简单 | 解释什么是 Python 虚拟环境 | `auto` | Simple v1 | 基础概念解释 |
| M1 | 中等 | 搜索异常处理最佳实践并整理注意事项 | `auto` | 理想 Simple，允许 Complex | 多步线性 |
| M2 | 中等 | 统计项目中 Python/Markdown 文件数量并对比 | `auto` | 理想 Simple，允许 Complex | 文件遍历 + 简单分析 |
| M3 | 中等 | 搜索 LangGraph 相关博客并列出要点 | `auto` | 理想 Simple，允许 Complex | 轻量研究 +总结 |
| M4 | 中等 | 带条件分支的仓库分析任务 | `auto` | 理想 Complex，可能 Simple | 条件 + 分支 |
| H1 | 困难 | 调研并实现 asyncio 并发下载示例 | `auto` | Complex v2 | 多阶段研究+实现+验证 |
| H2 | 困难 | 基于 GitHub stars 的条件分支分析 | `auto` | Complex v2 | 条件分支 + 回退策略 |
| H3 | 困难 | 构造带 ZeroDivisionError 的脚本并自愈 | `auto` | Complex v2 | 故意失败 + 修复 |
| H4 | 困难 | 并行分析 docs 文档并生成重组方案 | `auto` | Complex v2 | 多文档并行 + 汇总 |

> 说明：对于中等难度用例，**理想路径**一列描述了人类直觉上更合适的路径。如果实际运行时偶尔路由到另一条路径，只要规划结构和最终结果合理，可视为可接受行为，但可以在文档中备注观察到的差异。

---

## 四、统一用例模板

后续所有用例都采用以下模板描述，便于人工执行与对比：

- **用例 ID & 标题**
- **难度**：简单 / 中等 / 困难
- **任务描述**：推荐直接可复制到 CLI 的中文 prompt
- **运行方式**：
  - 交互模式示例
  - 单次任务模式示例
- **推荐 PLAN_MODE**：通常为 `auto`，必要时说明如何强制 simple / complex
- **预期路由与计划形态**：
  - Simple v1 / Complex v2 DAG
  - 预期步数或大致层级结构
- **关键观测点**：
  - CLI 中建议重点关注的输出元素列表
- **评价标准（从 3 个维度）**：
  - 规划正确性
  - 执行速度（相对主观）
  - 结果质量

---

## 五、简单难度用例（Simple Path 验证）

### S1：查询上海今天天气

- **难度**：简单
- **任务描述**：
  - `查询上海今天天气`
- **运行方式**：
  - 交互模式：在提示符下输入上述中文任务并回车。
  - 单次任务模式：`PLAN_MODE=auto python main.py "查询上海今天天气"`
- **推荐 PLAN_MODE**：`auto`
- **预期路由与计划形态**：
  - `_rule_classify()` 应给出明显的 simple 判定（文本很短、动作单一）。
  - 走 **Simple v1 扁平计划**，计划包含约 1–2 个步骤（搜索 + 总结）。
- **关键观测点**：
  - 终端中显示：`Task complexity: Simple (v1 flat plan)`。
  - `Simple Plan (v1)` 表格中的步骤数与描述是否紧扣「查询天气」。
  - 工具调用中通常会使用 `web_search`（如果配置了真实搜索）或 mock 搜索。
- **评价标准**：
  - **规划正确性**：是否只包含必要的 1–2 步，无明显冗余步骤。
  - **执行速度**：整体耗时应非常短（主观感受为「秒级响应」）。
  - **结果质量**：回答中能清晰给出当天上海天气的核心信息（温度、降雨/晴朗等），即使基于 mock 结果也应表述清楚。

---

### S2：计算 12345+67890 的结果

- **难度**：简单
- **任务描述**：
  - `计算 12345+67890 的结果`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "计算 12345+67890 的结果"`
- **推荐 PLAN_MODE**：`auto`
- **预期路由与计划形态**：
  - 规则分类应判断为 **simple**。
  - 计划通常包含 1 个计算步骤，最多 2 步（理解任务 + 计算并给出结果）。
- **关键观测点**：
  - 路由应为 Simple v1。
  - 执行中可能使用 `execute_python` 进行实际计算，或直接在 LLM 内完成。
- **评价标准**：
  - **规划正确性**：是否没有不必要的 web 搜索等步骤。
  - **执行速度**：应为所有用例中最短之一。
  - **结果质量**：数值结果是否正确，最好能简单解释计算过程。

---

### S3：用一句话总结 Python 列表推导式的作用

- **难度**：简单
- **任务描述**：
  - `用一句话总结 Python 列表推导式的作用`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "用一句话总结 Python 列表推导式的作用"`
- **推荐 PLAN_MODE**：`auto`
- **预期路由与计划形态**：
  - 任务长度适中，动词较少，应被判为 simple。
  - 计划 2–3 步：必要时先简要搜索，再提炼一句话总结。
- **关键观测点**：
  - `Simple Plan (v1)` 中是否有清晰的「理解/搜索」→「总结」线性结构。
  - 执行日志中是否适度使用 `web_search`（根据模型习惯而定）。
- **评价标准**：
  - **规划正确性**：步骤数量和顺序是否合理，没有和任务无关的子任务。
  - **执行速度**：应接近 S1、S2，用时明显低于困难用例。
  - **结果质量**：一句话总结是否抓住「快速构建新列表」「基于现有可迭代对象做变换/过滤」等关键点。

---

### S4：将一句中文翻译为英文

- **难度**：简单
- **任务描述**：
  - `将这句话翻译为英文：我喜欢学习算法`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "将这句话翻译为英文：我喜欢学习算法"`
- **推荐 PLAN_MODE**：`auto`
- **预期路由与计划形态**：
  - 规则分类应判为 simple。
  - 计划通常 1–2 步：理解句子 → 翻译输出。
- **关键观测点**：
  - Simple 路径，无复杂依赖。
  - 终端中无 DAG 树展示，仅有简单计划表格和反思结果。
- **评价标准**：
  - **规划正确性**：没有多余步骤（例如不应出现文件读写）。
  - **执行速度**：和其他简单用例接近。
  - **结果质量**：翻译是否自然准确，例如 “I enjoy learning algorithms.”。

---

### S5：解释什么是 Python 虚拟环境

- **难度**：简单
- **任务描述**：
  - `解释什么是 Python 虚拟环境，以及它解决了什么问题（用 2–3 句话即可）`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "解释什么是 Python 虚拟环境，以及它解决了什么问题（用 2–3 句话即可）"`
- **推荐 PLAN_MODE**：`auto`
- **预期路由与计划形态**：
  - 规规则分类一般仍会判为 simple（文本中多为解释性短语）。
  - 计划 2–3 步：理解问题 → （可选）检索相关知识 → 组织解释。
- **关键观测点**：
  - Simple v1 计划下，是否只包含少量、清晰的解释步骤。
  - 反思阶段是否给出合理的质量评分和改进建议。
- **评价标准**：
  - **规划正确性**：步骤结构能否自然支撑一个简短解释任务。
  - **执行速度**：应明显快于复杂用例。
  - **结果质量**：能否说明「隔离依赖」「避免全局 Python 环境冲突」等核心动机。

---

## 六、中等难度用例（边界与混合场景）

中等难度用例主要用于验证：

- 规则分类器 `_rule_classify()` 的边界行为。
- LLM 兜底分类 `_llm_classify()` 是否只在必要时触发，且给出合理判定。
- Simple / Complex 两条路径在同一任务规模下的结构与速度差异。

### M1：搜索异常处理最佳实践并整理注意事项

- **难度**：中等
- **任务描述**：
  - `先搜索 Python 异常处理的最佳实践，然后整理成 3 条注意事项`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "先搜索 Python 异常处理的最佳实践，然后整理成 3 条注意事项"`
- **推荐 PLAN_MODE**：`auto`
- **理想路径**：
  - 规则评分可能为 **ambiguous**，触发 `_llm_classify()`。
  - 理想情况下被判为 **simple**：2–4 步扁平线性计划即可覆盖需求。
- **预期计划形态**：
  - 步骤示例：搜索资料 → 过滤/阅读 → 总结出 3 条注意事项。
- **关键观测点**：
  - 是否在日志中看到「Rule classifier: ambiguous, invoking LLM classifier」。
  - `Task complexity` 最终被判为 Simple 或 Complex。
  - Simple 情况下，是否仍能完整覆盖检索 + 总结流程。
- **评价标准**：
  - **规划正确性**：无论路由到 Simple 还是 Complex，计划是否覆盖「搜索→整理」两个核心阶段。
  - **执行速度**：Simple 通常更快；如果被判为 Complex，应看到更详细分解，但延迟略高。
  - **结果质量**：输出的 3 条注意事项是否具体、有指导价值。

---

### M2：统计项目中文件数量并生成对比总结

- **难度**：中等
- **任务描述**：
  - `分别统计当前 manus_demo 项目中 Python 文件和 Markdown 文件的数量，并输出一个对比总结`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "分别统计当前 manus_demo 项目中 Python 文件和 Markdown 文件的数量，并输出一个对比总结"`
- **推荐 PLAN_MODE**：`auto`
- **理想路径**：
  - 规则分类应给出 ambiguous，触发 LLM 兜底。
  - 理想情况下仍判为 **simple**，生成 3–5 步扁平计划（列出文件 → 过滤 → 计数 → 总结）。
- **预期计划形态**：
  - 行为上应大量使用 `file_ops` 工具（列目录、匹配扩展名）。
  - 可能有一定的内部循环或多次工具调用，但在计划层面仍是线性步骤。
- **关键观测点**：
  - 工具调用日志中 `file_ops` 使用频率。
  - 是否在同一 Super-step 中串行执行各统计操作（Simple 路径下不会使用 DAGExecutor）。
- **评价标准**：
  - **规划正确性**：是否将 Python 与 Markdown 统计清晰区分。
  - **执行速度**：受项目体量影响，但整体应仍然较快。
  - **结果质量**：计数是否与实际项目结构大致一致，描述是否清晰。

---

### M3：搜索 LangGraph 相关博客并列出要点

- **难度**：中等
- **任务描述**：
  - `搜索最近三篇关于 LangGraph 的中文博客，并用要点列出它们的主要思想`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "搜索最近三篇关于 LangGraph 的中文博客，并用要点列出它们的主要思想"`
- **推荐 PLAN_MODE**：`auto`
- **理想路径**：
  - 规则分类多半为 ambiguous。
  - 理想上可以走 Simple v1，也可能被判为 Complex（分解为多个子目标）。
- **预期计划形态**：
  - 至少包含：搜索博客 → 选取 3 篇 → 阅读并提炼要点 → 汇总输出 3 组要点。
- **关键观测点**：
  - 如果走 Complex 路径，是否自动将「处理每篇博客」拆为多个 Action 并并行执行。
  - 最终结果中是否有清晰的分条要点列表。
- **评价标准**：
  - **规划正确性**：是否覆盖选择、阅读和提炼 3 个阶段。
  - **执行速度**：Simple 一般更快；Complex 会更加细致、对并行更友好。
  - **结果质量**：要点是否抓住 LangGraph 的核心理念（DAG、Super-step、状态管理等）。

---

### M4：带条件分支的仓库分析任务

- **难度**：中等
- **任务描述**：
  - `搜索一个 Python 开源项目，如果它的 GitHub stars 大于 5000，就分析最近 10 个 issue 的常见问题类型并输出统计；如果找不到满足条件的项目，则改为分析一个 stars 大于 1000 的备选项目`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：`PLAN_MODE=auto python main.py "搜索一个 Python 开源项目，如果它的 GitHub stars 大于 5000，就分析最近 10 个 issue 的常见问题类型并输出统计；如果找不到满足条件的项目，则改为分析一个 stars 大于 1000 的备选项目"`
- **推荐 PLAN_MODE**：`auto`
- **理想路径**：
  - 规则分类因包含「如果」「则」等条件词，得分偏向复杂，理想情况下被判为 **complex**。
  - 但由于任务规模仍有限，被判为 simple 也可以接受。
- **预期计划形态**：
  - Complex 路径下，DAG 中应出现表示「主仓库分析」与「备选仓库分析」的条件/分支结构。
  - 可能通过条件边决定是否执行备选路径。
- **关键观测点**：
  - DAG 树中是否能看出条件分支节点。
  - 执行日志中，当主分支条件不满足时，下游节点是否被 SKIPPED。
- **评价标准**：
  - **规划正确性**：条件与分支是否被清晰建模，避免重复或遗漏分析。
  - **执行速度**：相对简单用例略慢，但应能利用并行处理部分子任务。
  - **结果质量**：无论最终分析的是主仓库还是备选仓库，统计结果是否结构化清晰。

---

## 七、困难难度用例（复杂 DAG 与自愈场景）

困难用例聚焦于：

- **多阶段任务**：调研 → 设计 → 实现 → 验证。
- **复杂依赖与并行**：多个 SubGoal、多个 Action 并行执行。
- **错误与回滚**：主动制造失败，观察失败处理和局部重规划。

### H1：调研并实现 asyncio 并发下载示例

- **难度**：困难
- **任务描述**：
  - `调研当前主流的 Python 并发模型（线程、多进程、asyncio），分别总结各自优缺点，并在本项目中新建一个示例脚本，使用 asyncio 实现并发下载多个网页（例如 3–5 个公开站点），运行代码验证输出是否正确，最后用一段文字比较该示例与传统多线程实现的差异。`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：将整段描述作为一个字符串传给 `python main.py "..."`。
- **推荐 PLAN_MODE**：`auto`（强烈建议保持自动，以测试复杂路径路由能力）
- **预期路径与计划形态**：
  - 规则分类应直接判为 **complex**（文本较长，动词与阶段明显较多）。
  - 走 **Complex v2 DAG** 路径，DAG 中至少包含：
    - 调研并发模型（SubGoal）
    - 设计 asyncio 示例（SubGoal）
    - 编写并运行示例代码（SubGoal）
    - 对比并撰写总结（SubGoal）
- **关键观测点**：
  - DAG 树中是否有多个 SubGoal，且部分调研任务在同一 Super-step 中并行执行。
  - `execute_python` 和 `file_ops` 是否被用于创建和运行示例脚本。
  - 如果示例代码第一次运行失败，是否触发局部重规划或重试。
- **评价标准**：
  - **规划完整性**：是否覆盖调研、实现与验证三个核心阶段。
  - **执行速度**：明显慢于简单用例，但应能通过并行调研环节缩短总时间。
  - **结果质量**：示例代码是否可运行，比较是否指出 asyncio 在 I/O 密集场景的优势。

---

### H2：基于 GitHub stars 的条件分支分析

- **难度**：困难
- **任务描述**：
  - `围绕 Python Web 框架，选择一个 GitHub stars 数大于 20000 的项目作为主研究对象；如果在可用搜索结果中没有满足条件的项目，则选择 stars 大于 5000 的备选项目。对选定项目的最近 20 个 issue 进行分类，统计常见问题类型，并生成一个 Markdown 报告文件，包含：项目简介、stars 情况、常见问题类型分布表，以及你对该生态稳定性的评价。`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：整段描述传给 `python main.py "..."`。
- **推荐 PLAN_MODE**：`auto`
- **预期路径与计划形态**：
  - 规则分类应非常倾向于 **complex**。
  - DAG 中应包含：
    - 搜索并选择主项目（含 stars 条件）。
    - 备选项目路径（条件分支）。
    - 抓取并分析 issue。
    - 生成 Markdown 报告（`file_ops` 写文件）。
- **关键观测点**：
  - 条件分支在 DAG 中的体现（条件边、SKIPPED 节点）。
  - 如果搜索结果不稳定，观察实际执行路径是主项目还是备选项目。
  - 报告文件是否按要求出现在工作目录中。
- **评价标准**：
  - **规划完整性**：条件逻辑和多阶段分析是否完整。
  - **执行速度**：复杂任务，允许较长执行时间，但不应出现明显「空转」步骤。
  - **结果质量**：报告结构是否清晰，是否给出了合理的生态评价。

---

### H3：构造带 ZeroDivisionError 的脚本并自愈

- **难度**：困难
- **任务描述**：
  - `在当前 manus_demo 项目中新建一个 Python 文件 demo_bug.py，其中包含一个故意触发 ZeroDivisionError 的函数和一个 main 入口来调用它；运行该文件以观察错误日志；然后修改代码修复该错误（例如通过参数检查或异常处理），再次运行验证通过；最后在结果中解释这个 bug 的原因和修复思路。`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：整段描述传给 `python main.py "..."`。
- **推荐 PLAN_MODE**：`auto`
- **预期路径与计划形态**：
  - 必须走 Complex v2 路径，DAG 中包含：
    - 创建带缺陷的脚本。
    - 运行脚本并捕获错误。
    - 分析错误并修改代码。
    - 再次运行验证。
    - 输出总结说明。
- **关键观测点**：
  - 某个 Action 节点在第一次运行时应失败（RUNNING → FAILED），下游可能触发回滚或局部重规划。
  - 失败子树是否被合理替换或重试，后续状态是否最终达到 COMPLETED。
  - 反思阶段是否能识别到「先失败后修复」这一过程，并给出正向评价。
- **评价标准**：
  - **规划完整性**：是否显式包含「构造 bug」「运行失败」「修复」「验证」多个阶段。
  - **执行速度**：多次执行代码会拉长时间，这是预期行为。
  - **结果质量**：总结中是否解释 ZeroDivisionError 的根因，以及采用了何种修复策略。

---

### H4：并行分析 docs 文档并生成重组方案

- **难度**：困难
- **任务描述**：
  - `针对 manus_demo 仓库下的 docs 目录，完成以下任务：1）读取主要文档（例如 hybrid-plan-routing-v4.md、data-structures-and-algorithms.md、planning-gap-analysis.md、upgrade-plan-v3.md），提取每篇文档的标题和一级小节标题；2）对比这些文档覆盖的主题差异与重叠；3）给出一个建议的 docs 目录重组方案，并将分析结果写入一个新的 Markdown 文件 docs/docs-reorg-proposal.md。要求在规划时尽量并行处理各文档的解析，然后再进行汇总分析。`
- **运行方式**：
  - 交互模式输入上述任务。
  - 单次任务模式：整段描述传给 `python main.py "..."`。
- **推荐 PLAN_MODE**：`auto`
- **预期路径与计划形态**：
  - 规则分类应判为 complex，走 DAG 路径。
  - 理想 DAG 结构：
    - SubGoal A：并行解析多篇文档（多个 Action，可在同一 Super-step 并行）。
    - SubGoal B：对比主题差异与重叠。
    - SubGoal C：生成重组方案并写入新文档。
- **关键观测点**：
  - Super-step 日志中，多个解析文档的 Action 是否在同一轮被调度执行。
  - `file_ops` 是否被频繁用于读取原文档和写入 `docs/docs-reorg-proposal.md`。
  - 反思阶段是否评价并行规划的合理性。
- **评价标准**：
  - **规划完整性**：是否显式拆出「解析」「对比」「重组方案」三个阶段。
  - **执行速度**：相对于串行解析，是否可以观察到并行带来的一定加速（主观感受）。
  - **结果质量**：生成的重组方案是否合理，并且新文档结构是否清晰。

---

## 八、执行建议与后续扩展

- **执行顺序建议**：
  - 第一次使用时，建议按顺序执行：S1–S5 → M1–M4 → H1–H4。
  - 在执行同一难度层的多个用例时，可以关注：
    - 规划结构的一致性（是否遵循同一套设计理念）。
    - 执行速度的相对差异（例如文件操作 vs 代码执行 vs 纯推理）。
- **调试建议**：
  - 对困难用例，建议增加 `-v` 或 `--verbose`，以获得更详细日志，便于分析分类和重规划行为。
  - 若希望单独验证某条路径的行为，可临时设置 `PLAN_MODE=simple` 或 `PLAN_MODE=complex` 强制路由。
- **可能的自动化扩展方向**（本仓库当前不实现，仅作思路留档）：
  - 在测试框架中封装调用 `OrchestratorAgent.run()` 的基准测试，记录：
    - 规划阶段耗时
    - 执行阶段耗时与 Super-step 次数
    - LLM 调用 token 统计
  - 对上述用例建立「黄金结果」摘要，自动对比规划结构和关键字段是否保持稳定。

